"""
Streamlit RAG Chatbot Application

Features:
- Chat interface (vLLM + Qdrant)
- Sidebar for document management (Incremental Upload/Delete)
- Dynamic vectorstore updates
- System status indicators
"""

import streamlit as st
import os
import sys

# Add project root to path for imports
sys.path.append(os.path.abspath(".."))

from src.loader import load_single_document
from src.splitter import split_documents
from src.vectorstore import create_embeddings, create_vectorstore, add_documents_to_collection, delete_from_collection
from src.llm import create_llm
from src.retriever import create_retriever
from src.reranker import create_reranker
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Set up directories
DATA_DIR = os.path.abspath("../data")
QDRANT_PATH = os.path.abspath("../qdrant_db")
os.makedirs(DATA_DIR, exist_ok=True)

st.set_page_config(page_title="RAG AsistanÄ±", page_icon="ğŸ¤–", layout="wide")

# --- INITIALIZATION ---
@st.cache_resource
def initialize_rag():
    embeddings = create_embeddings()
    # Initialize vectorstore without docs first
    vectorstore = create_vectorstore(docs=[], embeddings=embeddings, path=QDRANT_PATH)
    llm = create_llm()
    
    # Initialize reranker (optional, can fail gracefully)
    reranker = None
    try:
        reranker = create_reranker(device="cuda")
    except Exception as e:
        print(f"Reranker yÃ¼klenemedi: {e}")
    
    return vectorstore, llm, embeddings, reranker

try:
    vectorstore, llm, embeddings, reranker = initialize_rag()
    st.session_state["rag_ready"] = True
except Exception as e:
    st.error(f"Sistem baÅŸlatÄ±lamadÄ±: {e}")
    st.stop()

# --- SIDEBAR ---
with st.sidebar:
    st.header("ğŸ“‚ Dosya ve Sistem YÃ¶netimi")
    
    # SYSTEM STATUS
    st.subheader("ğŸ“Š Sistem Durumu")
    if st.session_state.get("rag_ready"):
        st.success("ğŸŸ¢ VektÃ¶r DB: HazÄ±r (Qdrant)")
        st.info("ğŸŸ¢ Model: LLaMA vLLM (Aktif)")
    else:
        st.error("ğŸ”´ Sistem: BaÄŸlÄ± DeÄŸil")

    # RESET CHAT
    if st.button("ğŸ—‘ï¸ Sohbeti Temizle", use_container_width=True):
        st.session_state.messages = []
        st.rerun()
    
    st.divider()
    
    # MULTI-QUERY AYARI
    st.subheader("ğŸ” Arama AyarlarÄ±")
    use_multi_query = st.checkbox(
        "Multi-query (Daha Ä°yi Arama)",
        value=st.session_state.get("use_multi_query", False),
        help="Bir soruyu birden fazla ÅŸekilde ifade ederek arama yapar. Daha iyi sonuÃ§ verir ama daha yavaÅŸtÄ±r.",
        key="multi_query_checkbox"
    )
    st.session_state["use_multi_query"] = use_multi_query
    
    if use_multi_query:
        num_queries = st.slider(
            "Alternatif Soru SayÄ±sÄ±",
            min_value=2,
            max_value=5,
            value=st.session_state.get("num_queries", 3),
            help="Her soru iÃ§in kaÃ§ alternatif soru Ã¼retilecek",
            key="num_queries_slider"
        )
        st.session_state["num_queries"] = num_queries
    else:
        st.session_state["num_queries"] = 3  # VarsayÄ±lan (kullanÄ±lmayacak)
    
    # RE-RANK AYARI
    use_rerank = st.checkbox(
        "Re-ranking (Daha Ä°yi SÄ±ralama)",
        value=st.session_state.get("use_rerank", False),
        help="SonuÃ§larÄ± cross-encoder ile yeniden sÄ±ralar. %15-25 daha iyi accuracy saÄŸlar.",
        key="rerank_checkbox",
        disabled=reranker is None
    )
    st.session_state["use_rerank"] = use_rerank and reranker is not None
    
    if reranker is None:
        st.caption("âš ï¸ Reranker mevcut deÄŸil. 'sentence-transformers' paketini yÃ¼kleyin.")
    
    if use_rerank and reranker:
        rerank_top_n = st.slider(
            "Rerank iÃ§in DokÃ¼man SayÄ±sÄ±",
            min_value=10,
            max_value=50,
            value=st.session_state.get("rerank_top_n", 20),
            help="Reranking iÃ§in kaÃ§ dokÃ¼man alÄ±nacak (daha fazla = daha iyi ama daha yavaÅŸ)",
            key="rerank_top_n_slider"
        )
        st.session_state["rerank_top_n"] = rerank_top_n
    else:
        st.session_state["rerank_top_n"] = 20  # VarsayÄ±lan

    st.divider()

    # FILE UPLOAD
    st.subheader("ğŸ“„ Yeni DokÃ¼man Ekle")
    
    # Split method seÃ§imi
    split_method = st.radio(
        "BÃ¶lme YÃ¶ntemi:",
        ["Recursive (HÄ±zlÄ±)", "Semantic (Daha Ä°yi)"],
        index=0,
        help="Recursive: HÄ±zlÄ±, karakter bazlÄ± bÃ¶lme\nSemantic: YavaÅŸ ama anlamsal sÄ±nÄ±rlarda bÃ¶lme (daha iyi retrieval)"
    )
    use_semantic = split_method == "Semantic (Daha Ä°yi)"
    
    uploaded_files = st.file_uploader(
        "PDF veya TXT yÃ¼kleyin", 
        accept_multiple_files=True,
        type=['pdf', 'txt']
    )
    
    if uploaded_files:
        if st.button("YÃ¼kle ve Ä°ÅŸle", type="primary"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, uploaded_file in enumerate(uploaded_files):
                status_text.text(f"Ä°ÅŸleniyor: {uploaded_file.name}...")
                
                # 1. Save File
                file_path = os.path.join(DATA_DIR, uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                # 2. Load & Split
                docs = load_single_document(file_path)
                
                # Semantic veya Recursive splitter kullan
                if use_semantic:
                    chunks = split_documents(docs, method="semantic", embeddings=embeddings)
                else:
                    chunks = split_documents(docs, method="recursive")
                
                # 3. Add to Qdrant (Incremental)
                add_documents_to_collection(vectorstore, chunks)
                
                progress_bar.progress((i + 1) / len(uploaded_files))
            
            status_text.text("âœ… TÃ¼m dosyalar eklendi!")
            st.toast(f"{len(uploaded_files)} dosya vektÃ¶r veritabanÄ±na eklendi.")
            st.rerun()

    st.divider()
    
    # FILE LIST & DELETE
    st.subheader("ğŸ“š KayÄ±tlÄ± DokÃ¼manlar")
    files = os.listdir(DATA_DIR)
    if files:
        for file in files:
            col1, col2 = st.columns([0.8, 0.2])
            with col1:
                st.text(f"ğŸ“„ {file}")
            with col2:
                if st.button("âŒ", key=f"del_{file}"):
                    # 1. Delete from Qdrant
                    file_path = os.path.join(DATA_DIR, file)
                    delete_from_collection(vectorstore, file_path)
                    
                    # 2. Delete file
                    os.remove(file_path)
                    st.toast(f"{file} silindi.")
                    st.rerun()
    else:
        st.info("HenÃ¼z dokÃ¼man yok.")

# --- MAIN CHAT ---
st.title("ğŸ¤– RAG AsistanÄ±")
st.caption("LLaMA 3.1 & Qdrant ile gÃ¼Ã§lendirilmiÅŸ dokÃ¼man asistanÄ±")

if "messages" not in st.session_state:
    st.session_state.messages = []

# Display history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Retrieval Chain
def get_response(question, use_multi_query=False, num_queries=3, use_rerank=False, reranker=None, rerank_top_n=20):
    retriever = create_retriever(
        vectorstore, 
        strategy="auto", 
        question=question,
        use_multi_query=use_multi_query,
        llm=llm if use_multi_query else None,
        num_queries=num_queries,
        use_rerank=use_rerank and reranker is not None,
        reranker=reranker,
        rerank_top_n=rerank_top_n
    )
    
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""
Sen yardÄ±mcÄ± bir asistansÄ±n.
Soru: {question}
BaÄŸlam: {context}

Cevap (TÃ¼rkÃ§e, kÄ±sa ve Ã¶z):
"""
    )
    
    chain = (
        {
            "question": RunnablePassthrough(),
            "context": retriever
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return chain.stream(question)

# Input
if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # Arama ayarlarÄ±nÄ± session state'den al (sidebar'da ayarlanÄ±yor)
            use_mq = st.session_state.get("use_multi_query", False)
            num_q = st.session_state.get("num_queries", 3)
            use_rr = st.session_state.get("use_rerank", False)
            rerank_top = st.session_state.get("rerank_top_n", 20)
            
            stream = get_response(
                prompt, 
                use_multi_query=use_mq, 
                num_queries=num_q,
                use_rerank=use_rr,
                reranker=reranker,
                rerank_top_n=rerank_top
            )
            for chunk in stream:
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
        except Exception as e:
            st.error(f"Hata: {e}")
            full_response = "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu."
        
        st.session_state.messages.append({"role": "assistant", "content": full_response})
